{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb73ded-c374-4fc8-a750-1d02246e5e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\iason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-05 14:07:04 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import emot\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23815af2-bffc-4ee4-91a0-60f803b8ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-05 14:07:05 +02:00)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdbc9cb-c6d1-4484-906c-50f9fe0d751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-02-05 14:07:06 +02:00)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bf2108-490e-4a16-bce2-b946304a304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms (started: 2022-02-05 14:07:06 +02:00)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/train_gr/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17164706-b4ab-42d3-a733-98155002a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2022-02-05 14:07:07 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def clean_column(col):\n",
    "    '''\n",
    "    input --> column as series\n",
    "    clean from extra space characters, digits, links\n",
    "    return Series: the clean column\n",
    "    '''\n",
    "    rev = col.copy()\n",
    "    rev = (rev.str.replace(r'\\n|\\r|\\t', ' ', regex=True)\n",
    "           .str.replace(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', ' ', regex=True)\n",
    "           .str.replace(r'\\s+', ' ', regex=True)\n",
    "           .str.lower()\n",
    "          )\n",
    "    \n",
    "    return rev\n",
    "\n",
    "def tokenize_clean(col, stem=True, lemmatize=False):\n",
    "    '''\n",
    "    tokenize column and remove stopwords and punctuation\n",
    "    apply stemmer of lemmatizer\n",
    "    return tokens as a list of lists\n",
    "    '''\n",
    "    rev = col.copy()\n",
    "    tokens = rev.apply(word_tokenize)\n",
    "    # remove stopwords\n",
    "    words = stopwords.words(\"english\")\n",
    "    lambd = lambda s: [word for word in s if word not in words]\n",
    "    tokens = tokens.apply(lambd)\n",
    "    # remove punctuation\n",
    "    import string\n",
    "    punk = list(string.punctuation)\n",
    "    lambd = lambda s: [word for word in s if word not in punk + ['...']]\n",
    "    tokens = tokens.apply(lambd)\n",
    "    \n",
    "    if stem:\n",
    "        tokens = tokens.apply(lambda s: [SnowballStemmer(\"english\").stem(w) for w in s])\n",
    "                              \n",
    "    return tokens\n",
    "    \n",
    "def find_emoji(df, column):\n",
    "        #no multiples\n",
    "        #keep only meaning \n",
    "        #df['emoji']=df[column].apply(emot_obj.emoji)\n",
    "        #df['emoticon']=df[column].apply(emot_obj.emoticons)\n",
    "        x={}\n",
    "        emot_obj = emot.core.emot()\n",
    "        for index,row in df.iterrows():\n",
    "            ej=set(emot_obj.emoji(row[column])['mean'])\n",
    "            et=set(emot_obj.emoticons(row[column])['mean'])\n",
    "            x[row['review_id']]=(list(ej)+list(et))\n",
    "        emojies = pd.Series(x)\n",
    "        return pd.DataFrame(emojies).reset_index().rename({'index': 'review_id', 0: 'emoticon'}, axis=1)\n",
    "    \n",
    "def preprocess_dataframe(path='../data/train_gr/train.csv'):\n",
    "    '''\n",
    "    Load dataframe from path\n",
    "    Add columns for clean, tokenized and emojies\n",
    "    return the initial dataframe with extra columns\n",
    "    '''\n",
    "    df = pd.read_csv(path)\n",
    "    df['clean'] = clean_column(df.user_review)\n",
    "    df['tokens'] = tokenize_clean(df.clean)\n",
    "    \n",
    "    # find emojies and merge them into dataframe\n",
    "    emojies = find_emoji(df, 'user_review')\n",
    "    df = df.merge(emojies, on='review_id', how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc85f23a-bb4a-43e2-acd4-0d2b290220fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 34s (started: 2022-02-05 14:07:08 +02:00)\n"
     ]
    }
   ],
   "source": [
    "df1 = preprocess_dataframe('../data/train_gr/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a761731-9028-4108-b9c8-a4a4c534e7c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'profanity_filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6332/1473637891.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprofanity_filter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfanityFilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProfanityFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"That's bullshit!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'profanity_filter'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 656 ms (started: 2022-02-05 14:08:43 +02:00)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089a835c-9010-478f-9411-ff75f269b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 s (started: 2022-02-05 14:10:04 +02:00)\n"
     ]
    }
   ],
   "source": [
    "df1.to_csv('../data/train_gr/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b81b5b2-75ba-454c-9b18-677bb8fc01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38 s (started: 2022-02-05 13:00:56 +02:00)\n"
     ]
    }
   ],
   "source": [
    "emojies = find_emoji(df, 'user_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72dbb67-13e1-4750-a780-653523266cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
